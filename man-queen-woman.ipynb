{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#### This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-29T18:49:56.644033Z","iopub.execute_input":"2022-07-29T18:49:56.645841Z","iopub.status.idle":"2022-07-29T18:49:56.672996Z","shell.execute_reply.started":"2022-07-29T18:49:56.645737Z","shell.execute_reply":"2022-07-29T18:49:56.671783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What if words were vectors ?","metadata":{}},{"cell_type":"markdown","source":"*Word2vec is a technique for natural language processing published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that a simple mathematical function (the cosine similarity between the vectors) indicates the level of semantic similarity between the words represented by those vectors.* **Wikipedia**","metadata":{}},{"cell_type":"markdown","source":"**Our dataset contains the representation of multiple english words as vectors.**\n\n**We can explore what the interesting consequences of viewing words as vectors are.**","metadata":{}},{"cell_type":"markdown","source":"# Data loading and processing","metadata":{}},{"cell_type":"markdown","source":"**we read the file and then output a dict that gives the index from the word**\n\n**a word from an index**\n\n**a numpy array containing a word vector per word**","metadata":{}},{"cell_type":"code","source":" def read_f(file):\n\n    vects = []\n    word_index, index_word = {}, {}\n    i = 0\n    lines = open(file)\n    next(lines)\n    for line in lines:\n        splt = line.strip().split()\n        vects.append(np.array(splt[1:], dtype=float))\n        word_index[splt[0]] = i\n        index_word[i] = splt[0]\n        i+=1\n\n    return word_index, index_word, vects\n\nword_index, index_word, vects = read_f('/kaggle/input/fast-text-embeddings-without-subwords/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec')","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:49:57.016917Z","iopub.execute_input":"2022-07-29T18:49:57.017975Z","iopub.status.idle":"2022-07-29T18:51:34.222073Z","shell.execute_reply.started":"2022-07-29T18:49:57.017934Z","shell.execute_reply":"2022-07-29T18:51:34.220528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vects = np.array(vects)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:51:34.224158Z","iopub.execute_input":"2022-07-29T18:51:34.224503Z","iopub.status.idle":"2022-07-29T18:51:36.437287Z","shell.execute_reply.started":"2022-07-29T18:51:34.224473Z","shell.execute_reply":"2022-07-29T18:51:36.436031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vects = np.stack(vects)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:51:36.453895Z","iopub.execute_input":"2022-07-29T18:51:36.454603Z","iopub.status.idle":"2022-07-29T18:51:40.312130Z","shell.execute_reply.started":"2022-07-29T18:51:36.454562Z","shell.execute_reply":"2022-07-29T18:51:40.311149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normalized = vects\nfor i in range((normalized.shape)[0]):\n    normalized[i] = normalized[i]/np.linalg.norm(normalized[i],2) ","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:51:40.345217Z","iopub.execute_input":"2022-07-29T18:51:40.345973Z","iopub.status.idle":"2022-07-29T18:51:53.751967Z","shell.execute_reply.started":"2022-07-29T18:51:40.345930Z","shell.execute_reply":"2022-07-29T18:51:53.750681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finding similar words","metadata":{}},{"cell_type":"markdown","source":"**Let's see how vector representation can help us to find similar words**","metadata":{}},{"cell_type":"code","source":"\ndef K_nearest_words_from_vector(vector, K=10):\n    nearest = []\n    cos_sim = np.dot(vector, normalized.T) \n    sorted_sim = cosine_similarities.argsort()\n    for k in range(1, K+1): \n        #We select the last k elements\n        index = sorted_sim[-k]\n        nearest.append((index_word[index], cos_sim[index]))\n    return nearest","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:51:53.753472Z","iopub.execute_input":"2022-07-29T18:51:53.753916Z","iopub.status.idle":"2022-07-29T18:51:53.761564Z","shell.execute_reply.started":"2022-07-29T18:51:53.753872Z","shell.execute_reply":"2022-07-29T18:51:53.760241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def K_nearest_words_from_word(word, K=10):\n    \"\"\"Pour un mot 'word' donné, trouve les K plus proches voisins sémantiques\"\"\"\n    index = word_index[word]\n    vector = normalized[index]\n    nearest = K_nearest_words_from_vector(vector)\n    return nearest","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:51:53.763338Z","iopub.execute_input":"2022-07-29T18:51:53.763811Z","iopub.status.idle":"2022-07-29T18:51:53.775267Z","shell.execute_reply.started":"2022-07-29T18:51:53.763767Z","shell.execute_reply":"2022-07-29T18:51:53.774398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" def show_words(closest_words):\n        \n        # Creating dataset\n        data = map(list, zip(*closest_words))\n        words = next(data)\n        similarity = next(data)\n        nb_words = len(words)\n        x_axis = np.array([i for i in range(nb_words)])\n        # Creating histogram\n        fig, ax = plt.subplots(1, 1, figsize=(10,5))\n        ax.bar(x_axis, np.array(similarity))\n\n        # Set title\n        ax.set_title(\"{} Closest words\".format(nb_words))\n\n        # adding labels\n        ax.set_xlabel('Similarity rank')\n        ax.set_ylabel('Similarity')\n\n        # Make some labels.\n        rects = ax.patches\n        labels = words\n\n        for rect, label in zip(rects, labels):\n            height = rect.get_height()\n            ax.text(rect.get_x() + rect.get_width() / 2, height+0.01, label,\n                    ha='center', va='bottom')","metadata":{"execution":{"iopub.status.busy":"2022-07-29T19:12:06.249734Z","iopub.execute_input":"2022-07-29T19:12:06.250515Z","iopub.status.idle":"2022-07-29T19:12:06.262075Z","shell.execute_reply.started":"2022-07-29T19:12:06.250473Z","shell.execute_reply":"2022-07-29T19:12:06.261095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's do some tests\nshow_words(K_nearest_words_from_word(\"book\")) \nshow_words(K_nearest_words_from_word(\"flat\")) # Didn't know flat eathers were so present \nshow_words(K_nearest_words_from_word(\"flying\"))","metadata":{"execution":{"iopub.status.busy":"2022-07-29T19:43:24.956460Z","iopub.execute_input":"2022-07-29T19:43:24.956914Z","iopub.status.idle":"2022-07-29T19:43:26.436841Z","shell.execute_reply.started":"2022-07-29T19:43:24.956881Z","shell.execute_reply":"2022-07-29T19:43:26.435719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word Arithmetic","metadata":{}},{"cell_type":"markdown","source":"**Here we see how vector operations can bring us some interesting analogies**","metadata":{}},{"cell_type":"code","source":"def analogies(w1, w2, w3):\n    return K_nearest_words_from_vector(normalized[word_index[w1]]+ normalized[word_index[w2]]-normalized[word_index[w3]])","metadata":{"execution":{"iopub.status.busy":"2022-07-29T19:30:13.364030Z","iopub.execute_input":"2022-07-29T19:30:13.364409Z","iopub.status.idle":"2022-07-29T19:30:13.370607Z","shell.execute_reply.started":"2022-07-29T19:30:13.364379Z","shell.execute_reply":"2022-07-29T19:30:13.369409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Our function creates analogies with the form : \"res is to w2 as w3 is to w1\"**","metadata":{}},{"cell_type":"markdown","source":"**For example:**","metadata":{}},{"cell_type":"code","source":"def get_best_analog(analogy):\n    return analogy[1][0]","metadata":{"execution":{"iopub.status.busy":"2022-07-29T19:37:16.787713Z","iopub.execute_input":"2022-07-29T19:37:16.788144Z","iopub.status.idle":"2022-07-29T19:37:16.795028Z","shell.execute_reply.started":"2022-07-29T19:37:16.788112Z","shell.execute_reply":"2022-07-29T19:37:16.793631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"{} is to queen as man is to woman \".format(get_best_analog(analogies('man','queen', 'woman'))))","metadata":{"execution":{"iopub.status.busy":"2022-07-29T19:37:38.681515Z","iopub.execute_input":"2022-07-29T19:37:38.681989Z","iopub.status.idle":"2022-07-29T19:37:38.985058Z","shell.execute_reply.started":"2022-07-29T19:37:38.681952Z","shell.execute_reply":"2022-07-29T19:37:38.983816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**You have the answer to the title of this notebook.**","metadata":{}},{"cell_type":"markdown","source":"If you liked this notebook feel free to upvote it :)\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}